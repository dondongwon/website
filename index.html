<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Dong Won Lee</title>

  <meta name="author" content="Dong Won Lee">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/dong/cmu_seal_icon.jpeg">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Dong Won Lee</name>
              </p>
              <p>
                Hi, My name is Dong Won. My advisors and friends usually call me “Don" :)
              </p>

              <p>I’m a PhD Student at <a href="https://www.mit.edu/">MIT</a> working on the foundations of multimodal social agents, where I focus on developing machine learning methods and evaluation for social intelligence. I am extremely grateful to be co-advised 
		      by <a href="https://www.media.mit.edu/people/cynthiab/overview/">Professor Cynthia Breazeal</a> in the <a href="https://www.media.mit.edu/groups/personal-robots/overview/"> Personal Robots Group</a> at the <a href="https://www.media.mit.edu/"> Media Lab</a>		      
		      and <a href="https://www.cs.cmu.edu/~morency/">Professor Louis-Philippe Morency</a> at the <a href="https://lti.cs.cmu.edu/"> Language Technologies Institute at CMU</a>. 
		 Prior to MIT, I graduated with a Master's in Machine Learning and Bachelor's in Statistics and Machine Learning from <a href="https://www.ml.cmu.edu/">Carnegie Mellon University</a>. 
              </p>
		    
		    
		
              <p style="text-align:center">
                <a href="mailto:dondongwonlee@gmail.com">Email</a> &nbsp/&nbsp
                <a href="https://github.com/dondongwon/website/blob/master/images/dong/resume_dongwonl_public.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=kHSpCIMAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
	            	<a href="https://www.linkedin.com/in/don-dong-won-lee-ab964b172/">Linkedin</a>  &nbsp/&nbsp
		      <a href="https://twitter.com/_dongwonlee">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/dondongwon/">Github</a>  &nbsp/&nbsp

	            	<a href="https://dongwonl.medium.com/">Medium</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/dong/dong_photo.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/dong/dong_photo.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <!--News-->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>News</heading>
            <br>
	     <strong>06/2024</strong>: I've started an research internship at Microsoft Research in NYC working on Human-Oriented AI! 
            <br>	
	     <strong>03/2024</strong>: Our paper on <a href="https://arxiv.org/abs/2403.11330" target="_blank">Improving Dialogue Agents by Decomposing One Global Explicit Annotation with Local Implicit Multimodal Feedback
 </a> is posted on Arxiv. This is a new work that extends alignment techniques such as RLHF for a single final score instead of intermediate annotations. To do this, we utilize multimodal signals as implicit rewards. 
            <br>		  
             <strong>08/2023</strong>: I'll be TA'ing and mentoring students for <a href="https://sites.google.com/media.mit.edu/mas-630-2023/home" target="_blank"> MIT MAS.630 Affective Computing and Ethics </a> taught by <a href="https://web.media.mit.edu/~picard/">Rosalind Picard</a> at the MIT Media Lab, 
		  where I'll be giving lectures on advancements in Machine Learning for Affective Computing and Social Intelligence in LLMs. 
		  We encourage interested MIT/Harvard students to 
		  <a href="https://docs.google.com/forms/d/e/1FAIpQLSdDDKlu5feRr-_dB46fFt7jbgWGjPex3ksBYnPQ1mzyo1_iZg/viewform" target="_blank"> sign up </a>
		  for our class to learn about the future of Socio-Emotional AI!
            <br>
	    <strong>07/2023</strong>: Our paper on <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Lee_Lecture_Presentations_Multimodal_Dataset_Towards_Understanding_Multimodality_in_Educational_Videos_ICCV_2023_paper.pdf" target="_blank">Lecture Presentations Multimodal Dataset: Towards Understanding Multimodality in Educational Videos </a>
		  is accepted at ICCV 2023!
            <br>
	    <strong>07/2023</strong>: Our paper on <a href="https://arxiv.org/pdf/2305.12369.pdf" target="_blank">HIINT: Historical, Intra-and Inter-personal Dynamics Modeling with Cross-person Memory Transformer </a>
		  is accepted at ICMI 2023!
            <br>
            <strong>04/2023</strong>: Our paper on <a href="https://www.ijcai.org/proceedings/2023/0433.pdf" target="_blank">Multipar-T: Multiparty-Transformer for Capturing Contingent Behaviors in
              Group Conversations</a> is accepted at IJCAI 2023!
            <br> 
            <strong>03/2023</strong>: Our proposal for the <a href="https://sites.google.com/cam.ac.uk/sai2023/" target="_blank">1st Workshop on Social and Affective Intelligence (SAI) </a> has been accepted at ACII 2023! Please consider submitting your work!
            <br> 
            <strong>03/2022</strong>: Our paper on <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Ahuja_Low-Resource_Adaptation_for_Personalized_Co-Speech_Gesture_Generation_CVPR_2022_paper.pdf" target="_blank">Low-resource Adaptation for Personalized Co-Speech Gesture Generation</a> is accepted at CVPR 2022.
            <br>
            <strong>07/2021</strong>: Our paper on <a href="https://dl.acm.org/doi/10.1145/3461615.3485408"> Crossmodal clustered contrastive learning: Grounding of spoken language to gestures</a> is accepted to GENEA Workshop @ ICMI 2021.
            <br>
            <strong>05/2021</strong>: We are organizing the <a href="http://sites.google.com/view/xs-anim">First Workshop on Crossmodal Social Animation </a> at ICCV 2021.
            <br>
            <strong>09/2020</strong>: Our paper on <a href="https://www.aclweb.org/anthology/2020.findings-emnlp.170.pdf" target="_blank">No Gestures Left Behind: Learning Relationships between Spoken Language and Freeform Gestures</a> is accepted at Findings at EMNLP 2020.
            <br>
            <strong>07/2020</strong>: Our paper on <a href="https://arxiv.org/abs/2007.12553" target="_blank">Style Transfer for Co-Speech Gesture Animation</a> is accepted at ECCV 2020
          </td>
        </tr>
      </tbody></table>




        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/dong/geli.png" alt="clean-usnob" width="160">
            </td>
            <td width="75%" valign="middle">
              <a href=https://arxiv.org/pdf/2403.11330.pdf>
                <papertitle>Improving Dialogue Agents by Decomposing One Global Explicit Annotation with Local Implicit Multimodal Feedback</papertitle>
              </a>
              <br>
              <strong>Dong Won Lee</strong>,
	      <a href="https://www.media.mit.edu/people/haewon/overview/">Hae Won Park</a>,
	      <a href="https://people.csail.mit.edu/yoonkim/">Yoon Kim</a>,
              <a href="https://www.media.mit.edu/people/cynthiab/overview/">Cynthia Breazeal</a>,
              <a href="https://www.cs.cmu.edu/~morency/">Louis-Philippe Morency</a>
              <br>

              <em>In Submission</em>, 2024
              <br>
              <a href="https://arxiv.org/pdf/2403.11330.pdf">paper</a>
							/
<!--               <a href="https://github.com/dondongwon/MLPDataset">code</a> -->
              <p>We introduce an approach named GELI, which automatically decomposes a single Global Explicit post-interaction score while incorporating Local Implicit feedback from multimodal signals to adapt a language model to become more conversational.


              </p>
            </td>
          </tr>
		
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/dong/mlp.png" alt="clean-usnob" width="160">
            </td>
            <td width="75%" valign="middle">
              <a href=https://arxiv.org/abs/2208.08080>
                <papertitle>Lecture Presentations Multimodal Dataset: Towards Understanding Multimodality in Educational Videos</papertitle>
              </a>
              <br>
              <strong>Dong Won Lee</strong>,
              <a href="http://chahuja.com/">Chaitanya Ahuja</a>,
              <a href="https://www.cs.cmu.edu/~pliang/">Paul Pu Liang</a>,
              <a href="https://www.linkedin.com/in/sanika-natu">Sanika Natu</a>,
              <a href="https://www.cs.cmu.edu/~morency/">Louis-Philippe Morency</a>
              <br>

              <em>ICCV</em>, 2023
              <br>
              <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Lee_Lecture_Presentations_Multimodal_Dataset_Towards_Understanding_Multimodality_in_Educational_Videos_ICCV_2023_paper.pdf">paper</a>
							/
              <a href="https://github.com/dondongwon/MLPDataset">code</a>
              <p>We introduce the Multimodal Lecture Presentations dataset and PolyViLT a multimodal transformer trained with a multi-instance learning loss. We propose a large-scale benchmark testing the capabilities of machine learning models in multimodal understanding of educational content.


              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/dong/hiint.png" alt="clean-usnob" width="160">
            </td>
            <td width="75%" valign="middle">
              <a href=https://arxiv.org/pdf/2305.12369.pdf>
                <papertitle>HIINT: Historical, Intra-and Inter-personal Dynamics Modeling with Cross-person Memory Transformer</papertitle>
              </a>
              <br>
              
              <a href="https://ybkim95.github.io/">Yubin Kim</a>,
	      <strong>Dong Won Lee</strong>,
	      <a href="https://www.cs.cmu.edu/~pliang/">Paul Pu Liang</a>,
	      <a href="https://www.media.mit.edu/people/sharifah/overview/">Sharifa Algohwinem</a>,
              <a href="https://www.media.mit.edu/people/cynthiab/overview/">Cynthia Breazeal</a>,
              <a href="https://www.media.mit.edu/people/haewon/overview/">Hae Won Park</a>
              <br>

              <em>ICMI</em>, 2023
              <br>
              <a href="https://arxiv.org/pdf/2305.12369.pdf">paper</a>						
              <p>We model the Historical, Intra-and Inter-personal (HIINT) Dynamics in conversation by incorporating memory modules in the Cross-person Memory Transformer to address temporal coherence and better represent the context of conversational behaviors.
              </p>
            </td>
          </tr>

		
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/dong/multiparty.png" alt="clean-usnob" width="160">
            </td>
            <td width="75%" valign="middle">
              <a href=https://www.ijcai.org/proceedings/2023/0433.pdf>
                <papertitle>Multipar-T: Multiparty-Transformer for Capturing Contingent Behaviors in
                  Group Conversations</papertitle>
              </a>
              <br>
              <strong>Dong Won Lee</strong>,
              <a href="https://ybkim95.github.io/">Yubin Kim</a>,
              <a href="https://web.media.mit.edu/~picard/">Rosalind Picard</a>,
              <a href="https://www.media.mit.edu/people/cynthiab/overview/">Cynthia Breazeal</a>,
              <a href="https://www.media.mit.edu/people/haewon/overview/">Hae Won Park</a>
              <br>

              <em>IJCAI</em>, 2023 (Oral)
              <br>
              <a href="https://www.ijcai.org/proceedings/2023/0433.pdf">paper</a>
							<!-- /
              <a href="https://github.com/dondongwon/MLPDataset">code</a> -->
              <p>We introduce a new transformer architecture to model contingent behaviors in multiparty group conversations.


              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/dong/neurips_2021.png" alt="clean-usnob" width="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Ahuja_Low-Resource_Adaptation_for_Personalized_Co-Speech_Gesture_Generation_CVPR_2022_paper.pdf">
                <papertitle>Low-resource Adaptation for Personalized Co-Speech Gesture Generation</papertitle>
              </a>
              <br>
              <a href="http://chahuja.com/">Chaitanya Ahuja</a>, <strong>Dong Won Lee</strong>,
              <a href="https://www.cs.cmu.edu/~morency/">Louis-Philippe Morency</a>
              <br>

              <em>CVPR</em>, 2022
              <br>
              <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Ahuja_Low-Resource_Adaptation_for_Personalized_Co-Speech_Gesture_Generation_CVPR_2022_paper.pdf">paper</a>
							/
              <!-- <a href="https://github.com/chahuja/aisle">code</a> -->
              <p> We propose a new approach in crossmodal generative modeling in low-resource settings in the hopes to  to create a personalized gesture generation model (e.g. as part of a personalized avatar) with limited data from a new speaker.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/dong/icmi_2021.png" alt="clean-usnob" width="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://dl.acm.org/doi/10.1145/3461615.3485408">
                <papertitle>Crossmodal clustered contrastive learning: Grounding of spoken language to gestures</papertitle>
              </a>
              <br>
              <strong>Dong Won Lee</strong>,  <a href="http://chahuja.com/">Chaitanya Ahuja</a>,
              <a href="https://www.cs.cmu.edu/~morency/">Louis-Philippe Morency</a>
              <br>
              <em>ICMI, GENEA Workshop</em>, 2021
              <br>
              <a href="https://dl.acm.org/doi/10.1145/3461615.3485408">paper</a>
		    		    			/
		<a href="https://youtu.be/L5dHXTpCkeI">presentation video</a>
							/
              <a href="https://github.com/dondongwon/CC_NCE_GENEA">code</a>
              <p>We propose a new crossmodal contrastive learning loss to encourage a stronger grounding between gestures and spoken language.
              </p>
            </td>
          </tr>



          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/dong/emnlp_2020.png" alt="clean-usnob" width="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://www.aclweb.org/anthology/2020.findings-emnlp.170.pdf">
                <papertitle>No Gestures Left Behind: Learning Relationships between Spoken Language and Freeform Gestures</papertitle>
              </a>
              <br>
              <a href="http://chahuja.com/">Chaitanya Ahuja</a>, <strong>Dong Won Lee</strong>, <a href="https://sites.google.com/view/ryoishii/">Ryo Ishii</a>,
              <a href="https://www.cs.cmu.edu/~morency/">Louis-Philippe Morency</a>
              <br>
              <em>EMNLP, Findings</em>, 2020
              <br>
              <a href="https://www.aclweb.org/anthology/2020.findings-emnlp.170.pdf">paper</a>
							/
              <a href="https://github.com/chahuja/aisle">code</a>
              <p>We study relationships between spoken language and co-speech gestures to account for the long tail of text-gesture distribution.
              </p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/dong/eccv_2020.png" alt="clean-usnob" width="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2007.12553">
                <papertitle>Style Transfer for Co-Speech Gesture Animation: A Multi-Speaker Conditional Mixture Approach</papertitle>
              </a>
              <br>
              <a href="http://chahuja.com/">Chaitanya Ahuja</a>, <strong>Dong Won Lee</strong>, <a href="http://www.ci.seikei.ac.jp/nakano/index_e.html">Yukiko I. Nakano</a>,
              <a href="https://www.cs.cmu.edu/~morency/">Louis-Philippe Morency</a>
              <br>
              <em>ECCV</em>, 2020
              <br>
              <a href="http://chahuja.com/mix-stage">project page</a>
              /
              <a href="https://arxiv.org/abs/2007.12553">paper</a>
							/
              <a href="https://github.com/chahuja/mix-stage">code</a>
              <p>We propose a new style transfer model to learn individual styles of speaker's gestures.</p>
            </td>
          </tr>

        </tbody></table>


        <!--Resources-->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Resources</heading>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/dong/pats.png" alt="clean-usnob" width="160">
            </td>
            <td width="75%" valign="middle">
              <!-- <a href="https://arxiv.org/abs/2007.12553"> -->
                <papertitle> PATS Dataset: Pose, Audio, Transcripts and Style</papertitle>
              <!-- </a> -->
              <br>
              <a href="http://chahuja.com/">Chaitanya Ahuja</a>, <strong>Dong Won Lee</strong>, <a href="http://www.ci.seikei.ac.jp/nakano/index_e.html">Yukiko I. Nakano</a>,
              <a href="https://www.cs.cmu.edu/~morency/">Louis-Philippe Morency</a>
              <br>
              <a href="https://chahuja.com/pats/">dataset page</a>
              /
              <a href="http://chahuja.com/pats/download.html">download Link</a>
							/
              <a href="https://github.com/chahuja/pats">code</a>
              <p>PATS was collected to study correlation of co-speech gestures with audio and text signals. The dataset consists of a diverse and large amount of aligned pose, audio and transcripts.</p>
            </td>
          </tr>

        </tbody></table>


        <!--Teaching & More-->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td style="padding-bottom:10px">
              <heading>Teaching</heading>
              <!-- <p>
                Write some stuff here
              </p> -->
            </td>
          </tr>
        </tbody></table>


	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding-left:20px;padding-right:0px;padding-top:10px;padding-bottom:10px;width:10%;vertical-align:middle">
              <img src="./images/dong/affectivecomputing.png" style="width:100%;max-width:100%">
            </td>
            <td style="padding:20px;width:90%;vertical-align:middle">
              <a href="https://sites.google.com/media.mit.edu/mas-630-2023/home" target="_blank"> <papertitle>MIT MAS.630: Advanced Seminar: Affective  Computing and Ethics </papertitle> </a>
              <br>Graduate TA, Fall 2023 <br>
            </td>
          </tr>
        </tbody></table>

	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding-left:20px;padding-right:0px;padding-top:10px;padding-bottom:10px;width:10%;vertical-align:middle">
              <img src="./images/dong/LTI_logo.png" style="width:100%;max-width:100%">
            </td>
            <td style="padding:20px;width:90%;vertical-align:middle">
              <a href="https://yonatanbisk.com/teaching/mmml-s22/" target="_blank"> <papertitle>CMU LTI 11-777: Multimodal Machine Learning </papertitle> </a>
              <br>Graduate TA, Spring 2022 <br>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding-left:20px;padding-right:0px;padding-top:10px;padding-bottom:10px;width:10%;vertical-align:middle">
              <img src="./images/dong/mld.jpg" style="width:100%;max-width:100%">
            </td>
            <td style="padding:20px;width:90%;vertical-align:middle">
              <a href="http://www.andrew.cmu.edu/user/yuanzhil/cov.html" target="_blank"> <papertitle>CMU MLD 10-725: Convex Optimization</papertitle> </a>
              <br>Graduate TA, Spring 2021 <br>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding-left:20px; width:100px;vertical-align:middle">
              <img src="./images/dong/statds_logo.png" style="width:100%;max-width:100%">
            </td>
            <td style="padding:20px;width:90%;vertical-align:middle">
              <a href="http://www.stat.cmu.edu/" target="_blank"> <papertitle>CMU Stat & DS 36-202: Statistics & Data Science Methods </papertitle> </a>
              <br>Undergraduate TA, Fall 2019, Spring 2020, Fall 2020 (3 Semesters)<br>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding-left:20px; width:100px;vertical-align:middle">
              <img src="./images/dong/statds_logo.png" style="width:100%;max-width:100%">
            </td>
            <td style="padding:20px;width:90%;vertical-align:middle">
              <a href="http://www.stat.cmu.edu/" target="_blank"> <papertitle>CMU Stat & DS 36-200: Reasoning with Data</papertitle> </a>
              <br>Undergraduate TA, Fall 2020, Spring 2021 (2 Semesters) <br>
            </td>
          </tr>
        </tbody></table>



        <!--Services-->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td style="padding-bottom:10px">
              <heading>Services</heading>
              <!-- <p>
                Write some stuff here
              </p> -->
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding-left:20px;padding-right:20px;padding-top:10px;padding-bottom:10px;width:10%;vertical-align:middle">
              <img src="./images/dong/emnlp_logo.png" style="width:100%;max-width:100%">
            </td>
            <td style="padding:20px;width:90%;vertical-align:middle">
              <a href="https://2024.emnlp.org/" target="_blank"> <papertitle>Empirical Methods in Natural Language Processing (EMNLP 2021, 2024)</papertitle> </a>
              <br>Reviewer <br>
            </td>
          </tr>
        </tbody></table>

	      
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding-left:20px;padding-right:20px;padding-top:10px;padding-bottom:10px;width:10%;vertical-align:middle">
              <img src="./images/dong/acii2023.png" style="width:100%;max-width:100%">
            </td>
            <td style="padding:20px;width:90%;vertical-align:middle">
              <a href="https://sites.google.com/cam.ac.uk/sai2023/home" target="_blank"> <papertitle>Social and Affective Intelligence (SAI) @ ACII 2023 </papertitle> </a>
              <br>Co-Organizing Chair<br>
              <a href="https://sites.google.com/cam.ac.uk/sai2023/home">workshop page</a>
              
            </td>
          </tr>
        </tbody></table>


	      
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding-left:20px;padding-right:20px;padding-top:10px;padding-bottom:10px;width:10%;vertical-align:middle">
              <img src="./images/dong/cvf_logo.jpg" style="width:100%;max-width:100%">
            </td>
            <td style="padding:20px;width:90%;vertical-align:middle">
              <a href="http://sites.google.com/view/xs-anim" target="_blank"> <papertitle>First Workshop on Crossmodal Social Animation @ ICCV 2021 </papertitle> </a>
              <br>Publication Chair<br>
              <a href="http://sites.google.com/view/xs-anim">workshop page</a>
              /
              <a href="https://www.youtube.com/watch?v=zRA9tRSH7Uo">video</a>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding-left:20px;padding-right:20px;padding-top:10px;padding-bottom:10px;width:10%;vertical-align:middle">
              <img src="./images/dong/acm_logo.png" style="width:100%;max-width:100%">
            </td>
            <td style="padding:20px;width:90%;vertical-align:middle">
              <a href="https://icmi.acm.org/2021/" target="_blank"> <papertitle>International Conference on Multimodal Interaction (ICMI 2021)</papertitle> </a>
              <br>Reviewer<br>
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td style="padding-bottom:10px">
              <heading>Misc.</heading>
              <!-- <p>
                I spend some of my time writing about interesting things I come across while I study ML :)
              </p> -->
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="./images/dong/army.png" style="padding:25px;width:70%;max-width:100%">
            </td>
            <td style="width:90%;vertical-align:middle">
              <a href="https://www.youtube.com/watch?v=02NUQkjSqGI" target="_blank"> <papertitle> ROK Special Operations Unit Deployed in UAE </papertitle> </a>
              <br>Previously, I had the incredible opportunity to be a member of a South Korean special operations unit deployed to Abu Dhabi (AKH14).<br>
              Find me here: <a href="images/dong/dong_akh.jpeg"> photo</a> <br>

<!-- You can also find my name in this <a href="https://www.hankyung.com/politics/article/2018072445661"> article </a>, if you can read Korean :). -->
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Planned Submissions</heading>

            <br><br>
	    <strong>06/2024</strong>: EMNLP 2024 <br>
	    <br>



          </td>
        </tr>
      </tbody></table>



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Mentors</heading>
	    <p>
                I have been blessed to meet amazing mentors who have guided me to become a better researcher (and more importantly, a good person).
		    I believe that I can only repay what they've done for me by assisting others in their journey in any way I can.
		    Please don't hesitate to reach out!
              </p>

      <br><br>
	    <strong>Mentors and Advisors: (in Alphabetical Order) </strong>
	    <br>
	    <ul>
		  <li>Ben Eysenbach - CMU</li>
		  <li>Chaitanya Ahuja - CMU</li>
		  <li>Cynthia Breazeal - MIT</li>
		  <li>David Kosbie - CMU</li>
		  <li>Hae Won Park - MIT</li>
		  <li>Louis-Phillipe Morency - CMU</li>
		  <li>Mark Stehlik - CMU</li>
		  <li>Paul Pu Liang - CMU</li>
		  <li>Roz Picard - MIT</li>   
		  <li>Yoon Kim - MIT</li>  
		  <li>Sid Sen - Microsoft Research</li> 
	    </ul>
		  
		 
	    <br>



          </td>
        </tr>
      </tbody></table>








        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Website Credits Here: <a href="https://github.com/jonbarron/jonbarron_website">source code</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
