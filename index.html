<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Dong Won Lee</title>
  
  <meta name="author" content="Dong Won Lee">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/dong/cmu_seal_icon.jpeg">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Dong Won Lee</name>
              </p>
              <p>
                Hi, My name is Dong Won. My advisors and friends call me “Dong” or “Don”. Whatever is convenient for you!
              </p>
    
              <p>I’m a 5th-year Master's student in the <a href="https://www.ml.cmu.edu/">Machine Learning Department</a> 
                at <a href="https://www.cmu.edu/">Carnegie Mellon University</a>. (Expected Grad. May 2022). 
                I recently completed my B.S. in Statistics and Machine Learning at CMU, as well.
              </p>
              <p>
		          I am fascinated about designing ML models to understand the relationship between what we see (vision) and what we hear (language) 
              and implement these findings to interesting applications in education, human-robot-interaction, and  robotics control. 
              </p>
              <p style="text-align:center">
                <a href="mailto:dondongwonlee@gmail.com">Email</a> &nbsp/&nbsp
                <!-- <a href="https://github.com/dondongwonlee/">CV</a> &nbsp/&nbsp -->
                <a href="https://scholar.google.com/citations?user=kHSpCIMAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
	            	<a href="https://github.com/dondongwonlee/">Linkedin</a>  &nbsp/&nbsp
                <a href="https://github.com/dondongwon/">Github</a>  &nbsp/&nbsp
	            	<a href="https://dongwonl.medium.com/">Medium</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/dong/dong_photo.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/dong/dong_photo.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <!--News-->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>News</heading>
            <br><br>
            <strong>07/2021</strong>: Our paper on Crossmodal clustered contrastive learning: Grounding of spoken language to gestures is accepted to GENEA Workshop @ ICMI 2021.
            <br>
            <strong>06/2021</strong>: Graduated from my undergraduate degree at CMU!
            <br>
            <strong>05/2021</strong>: We are organizing the <a href="http://sites.google.com/view/xs-anim">First Workshop on Crossmodal Social Animation </a> at <a href="http://iccv2021.thecvf.com/">ICCV 2021</a>.
            <br>
            <strong>09/2020</strong>: Our paper on <a href="https://www.aclweb.org/anthology/2020.findings-emnlp.170.pdf" target="_blank">No Gestures Left Behind: Learning Relationships between Spoken Language and Freeform Gestures</a> is accepted at Findings at EMNLP 2020.
            <br>
            <strong>07/2020</strong>: Our paper on <a href="https://arxiv.org/abs/2007.12553" target="_blank">Style Transfer for Co-Speech Gesture Animation</a> is accepted at ECCV 2020
          </td>
        </tr>
      </tbody></table>




        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                During my Bachelor’s and Master’s, I’ve primarily worked in the <a href="http://multicomp.cs.cmu.edu/">MultiComp Lab</a>, 
                advised by <a href="https://www.cs.cmu.edu/~morency/"> Professor Louis-Philippe Morency</a>. 
                I'm currently dedicating my efforts to understand the fine-grained visual grounding between visual elements and language. 
                Prior, I focused on developing models in generating nonverbal behavioral cues (gestures) conditioned on language to improve naturalness in human-robot interaction. 
                
                I work with <a href="https://www.media.mit.edu/people/haewon/overview/">Dr. Hae Won Park</a> at the MIT Media Lab to predict people’s engagement with an agent using graphical models. 
                
                I am also collaborating with <a href="https://www.cs.cmu.edu/~rsalakhu/">Professor  Ruslan Salakhutinov</a>  to teach robots to learn optimal policies just with language.
              </p>
            </td>
          </tr>
        </tbody></table>  
	
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <!-- <tr onmouseout="mipnerf_stop()" onmouseover="mipnerf_start()"  bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='mipnerf_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/mipnerf_ipe_yellow.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/mipnerf_ipe_yellow.png' width="160">
              </div>
              <script type="text/javascript">
                function mipnerf_start() {
                  document.getElementById('mipnerf_image').style.opacity = "1";
                }

                function mipnerf_stop() {
                  document.getElementById('mipnerf_image').style.opacity = "0";
                }
                mipnerf_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://jonbarron.info/mipnerf">
                <papertitle>Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields</papertitle>
              </a>
              <br>
              <strong>Jonathan T. Barron</strong>,
              <a href="https://bmild.github.io/">Ben Mildenhall</a>,
              <a href="http://matthewtancik.com/">Matthew Tancik</a>, <br>
              <a href="https://phogzone.com/">Peter Hedman</a>,
              <a href="http://www.ricardomartinbrualla.com/">Ricardo Martin-Brualla</a>,
              <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>
              <br>
							<em>ICCV</em>, 2021 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
              <br>
              <a href="http://jonbarron.info/mipnerf">project page</a>
              /
              <a href="https://arxiv.org/abs/2103.13415">arXiv</a>
              /
              <a href="https://youtu.be/EpH175PY1A0">video</a>
							/
              <a href="https://github.com/google/mipnerf">code</a>
              <p></p>
              <p>NeRF is aliased, but we can anti-alias it by casting cones and prefiltering the positional encoding function.</p>
            </td>
          </tr>  -->

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/dong/icmi_2021.png" alt="clean-usnob" width="160">
            </td>
            <td width="75%" valign="middle">
              <!-- <a href="https://www.aclweb.org/anthology/2020.findings-emnlp.170.pdf"> -->
                <papertitle>Crossmodal clustered contrastive learning: Grounding of spoken language to gestures</papertitle>
              <!-- </a> -->
              <br>
              <strong>Dong Won Lee</strong>,  <a href="http://chahuja.com/">Chaitanya Ahuja</a>,
              <a href="https://www.cs.cmu.edu/~morency/">Louis-Philippe Morency</a> 
              <br>


              <em>ICMI GENEA Workshop</em>, 2021
              <br>
              <a href="https://openreview.net/pdf?id=o8CpxaBurZQ">paper</a>
							/
              <a href="https://github.com/dondongwon/CC_NCE_GENEA">code</a>
              <p>We propose a new crossmodal contrastive learning loss to encourage a stronger grounding between gestures and spoken language.
                

              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/dong/neurips_2021.png" alt="clean-usnob" width="160">
            </td>
            <td width="75%" valign="middle">
              <!-- <a href="https://www.aclweb.org/anthology/2020.findings-emnlp.170.pdf"> -->
                <papertitle>Low-Resource Adaptation of Spatio-Temporal Crossmodal Generative Models</papertitle>
              <!-- </a> -->
              <br>
              <a href="http://chahuja.com/">Chaitanya Ahuja</a>, <strong>Dong Won Lee</strong>, 
              <a href="https://www.cs.cmu.edu/~morency/">Louis-Philippe Morency</a> 
              <br>

              <em>In Submission</em>

              <!-- <em>ECCV</em>, 2020 -->
              <br>
              <!-- <a href="https://www.aclweb.org/anthology/2020.findings-emnlp.170.pdf">paper</a>
							/ -->
              <!-- <a href="https://github.com/chahuja/aisle">code</a> -->
              <!-- <p>We study relationships between spoken language and co-speech gestures to account for the long tail of text-gesture distribtution
                

              </p> -->
            </td>
          </tr>



          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/dong/emnlp_2020.png" alt="clean-usnob" width="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://www.aclweb.org/anthology/2020.findings-emnlp.170.pdf">
                <papertitle>No Gestures Left Behind: Learning Relationships between Spoken Language and Freeform Gestures</papertitle>
              </a>
              <br>
              <a href="http://chahuja.com/">Chaitanya Ahuja</a>, <strong>Dong Won Lee</strong>, <a href="https://sites.google.com/view/ryoishii/">Ryo Ishii</a>, 
              <a href="https://www.cs.cmu.edu/~morency/">Louis-Philippe Morency</a> 
              <br>
              <em>Findings at EMNLP</em>, 2020
              <br>
              <a href="https://www.aclweb.org/anthology/2020.findings-emnlp.170.pdf">paper</a>
							/
              <a href="https://github.com/chahuja/aisle">code</a>
              <p>We study relationships between spoken language and co-speech gestures to account for the long tail of text-gesture distribution.


              </p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/dong/eccv_2020.png" alt="clean-usnob" width="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2007.12553">
                <papertitle>Style Transfer for Co-Speech Gesture Animation: A Multi-Speaker Conditional Mixture Approach</papertitle>
              </a>
              <br>
              <a href="http://chahuja.com/">Chaitanya Ahuja</a>, <strong>Dong Won Lee</strong>, <a href="http://www.ci.seikei.ac.jp/nakano/index_e.html">Yukiko I. Nakano</a>, 
              <a href="https://www.cs.cmu.edu/~morency/">Louis-Philippe Morency</a> 
              <br>
              <em>ECCV</em>, 2020
              <br>
              <a href="http://chahuja.com/mix-stage">project page</a>
              /
              <a href="https://arxiv.org/abs/2007.12553">paper</a>
							/
              <a href="https://github.com/chahuja/mix-stage">code</a>
              <p>We propose a new style transfer model to learn individual styles of speaker's gestures.</p>
            </td>
          </tr>

        </tbody></table>


        <!--Resources-->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Resources</heading>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/dong/pats.png" alt="clean-usnob" width="160">
            </td>
            <td width="75%" valign="middle">
              <!-- <a href="https://arxiv.org/abs/2007.12553"> -->
                <papertitle> PATS Dataset: Pose, Audio, Transcripts and Style</papertitle>
              <!-- </a> -->
              <br>
              <a href="http://chahuja.com/">Chaitanya Ahuja</a>, <strong>Dong Won Lee</strong>, <a href="http://www.ci.seikei.ac.jp/nakano/index_e.html">Yukiko I. Nakano</a>, 
              <a href="https://www.cs.cmu.edu/~morency/">Louis-Philippe Morency</a> 
              <br>
              <a href="https://chahuja.com/pats/">dataset page</a>
              /
              <a href="http://chahuja.com/pats/download.html">download Link</a>
							/
              <a href="https://github.com/chahuja/pats">code</a>
              <p>PATS was collected to study correlation of co-speech gestures with audio and text signals. The dataset consists of a diverse and large amount of aligned pose, audio and transcripts.</p>
            </td>
          </tr>
          
        </tbody></table>


        <!--Teaching & More-->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td style="padding-bottom:10px">
              <heading>Teaching</heading>
              <!-- <p>
                Write some stuff here
              </p> -->
            </td>
          </tr>
        </tbody></table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding-left:20px;padding-right:0px;padding-top:10px;padding-bottom:10px;width:10%;vertical-align:middle">
              <img src="./images/dong/mld.jpg" style="width:100%;max-width:100%">
            </td>
            <td style="padding:20px;width:90%;vertical-align:middle">
              <a href="http://www.andrew.cmu.edu/user/yuanzhil/cov.html" target="_blank"> <papertitle>CMU MLD 10-725: Convex Optimization (PhD)</papertitle> </a>
              <br>Graduate TA, Spring 2021 <br>
            </td>
          </tr>
        </tbody></table>
          
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding-left:20px; width:100px;vertical-align:middle">
              <img src="./images/dong/statds_logo.png" style="width:100%;max-width:100%">
            </td>
            <td style="padding:20px;width:90%;vertical-align:middle">
              <a href="http://www.stat.cmu.edu/" target="_blank"> <papertitle>CMU Stat & DS 36-202: Statistics & Data Science Methods (Undergrad) </papertitle> </a>
              <br>Undergraduate TA, Fall 2019, Spring 2020, Fall 2020 (3 Semesters)<br>
            </td>
          </tr>
        </tbody></table> 
          
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding-left:20px; width:100px;vertical-align:middle">
              <img src="./images/dong/statds_logo.png" style="width:100%;max-width:100%">
            </td>
            <td style="padding:20px;width:90%;vertical-align:middle">
              <a href="http://www.stat.cmu.edu/" target="_blank"> <papertitle>CMU Stat & DS 36-200: Reasoning with Data (Undergrad)</papertitle> </a>
              <br>Undergraduate TA, Fall 2020, Spring 2021 (2 Semesters) <br>
            </td>
          </tr>
        </tbody></table> 
          
          

        <!--Services-->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td style="padding-bottom:10px">
              <heading>Services</heading>
              <!-- <p>
                Write some stuff here
              </p> -->
            </td>
          </tr>
        </tbody></table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding-left:20px;padding-right:20px;padding-top:10px;padding-bottom:10px;width:10%;vertical-align:middle">
              <img src="./images/dong/cvf_logo.jpg" style="width:100%;max-width:100%">
            </td>
            <td style="padding:20px;width:90%;vertical-align:middle">
              <a href="http://sites.google.com/view/xs-anim" target="_blank"> <papertitle>First Workshop on Crossmodal Social Animation @ ICCV 2021 </papertitle> </a>
              <br>Publication Chair<br>
            </td>
          </tr>
        </tbody></table>
          
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding-left:20px;padding-right:20px;padding-top:10px;padding-bottom:10px;width:10%;vertical-align:middle">
              <img src="./images/dong/acm_logo.png" style="width:100%;max-width:100%">
            </td>
            <td style="padding:20px;width:90%;vertical-align:middle">
              <a href="https://icmi.acm.org/2021/" target="_blank"> <papertitle>International Conference on Multimodal Interaction (ICMI 2021)</papertitle> </a>
              <br>Reviewer<br>
            </td>
          </tr>
        </tbody></table> 
          
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding-left:20px;padding-right:20px;padding-top:10px;padding-bottom:10px;width:10%;vertical-align:middle">
              <img src="./images/dong/emnlp_logo.png" style="width:100%;max-width:100%">
            </td>
            <td style="padding:20px;width:90%;vertical-align:middle">
              <a href="https://2021.emnlp.org/" target="_blank"> <papertitle>Empirical Methods in Natural Language Processing (EMNLP 2021)</papertitle> </a>
              <br>Reviewer <br>
            </td>
          </tr>
        </tbody></table> 

        <!--Machine Learning Blog-->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td style="padding-bottom:10px">
              <heading>Blog</heading>
              <p>
                I spend some of my time writing about interesting things I come across while I study ML :)
              </p>
            </td>
          </tr>
        </tbody></table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="./images/dong/medium_pos.png" style="width:100%;max-width:100%">
            </td>
            <td style="padding:20px;width:90%;vertical-align:middle">
              <a href="https://medium.com/swlh/elegant-intuitions-behind-positional-encodings-dc48b4a4a5d1" target="_blank"> <papertitle>Key Intuition Behind Positional Encodings </papertitle> </a>
              <br>Explaning the intuition behind the formulation of Transformer’s positional embeddings.<br>
            </td>
          </tr>
        </tbody></table>
          
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="./images/dong/medium_clearn.png" style="width:100%;max-width:100%">
            </td>
            <td style="padding:20px;width:90%;vertical-align:middle">
              <a href="https://dongwonl.medium.com/c-learning-no-reward-function-needed-for-goal-conditioned-rl-287a6a96e026" target="_blank"> <papertitle>C-Learning: No reward function needed for Goal-Conditioned RL</papertitle> </a>
              <br>A more-detailed summary of C-Learning: Learning to Achieve Goals via Recursive Classification, which studies the problem of predicting and controlling future states distribution of an autonomous agent.<br>
            </td>
          </tr>
        </tbody></table> 
	      
	      
	      
	
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td style="padding-bottom:10px">
              <heading>Misc.</heading>
              <!-- <p>
                I spend some of my time writing about interesting things I come across while I study ML :)
              </p> -->
            </td>
          </tr>
        </tbody></table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="./images/dong/army.png" style="padding:25px;width:70%;max-width:100%">
            </td>
            <td style="width:90%;vertical-align:middle">
              <a href="https://www.youtube.com/watch?v=02NUQkjSqGI" target="_blank"> <papertitle> ROK Special Operations Unit Deployed in UAE </papertitle> </a>
              <br>Previously, I had the incredible opportunity to be a member of a South Korean special operations unit deployed to Abu Dhabi (AKH14).<br>
              Please excuse the hideous sunglasses in this <a href="images/dong/dong_akh.jpeg"> photo</a>. 
		    
<!-- You can also find my name in this <a href="https://www.hankyung.com/politics/article/2018072445661"> article </a>, if you can read Korean :). -->
            </td>
          </tr>
        </tbody></table>
	      

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Planned Submissions</heading>
		  
            <br><br>
	    <strong>10/05/2021</strong>: ICLR 2022
		  
	    <br>   
            <strong>11/16/2021 or 11/17/2021</strong>: ACL 2022 or CVPR 2022
	    <br>
	    
    
       
          </td>
        </tr>
      </tbody></table>
	      
	      
	      
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Mentors</heading>
	    <p>
                I have been blessed to meet amazing mentors (listed beloo) who have guided me to become a better researcher (and a human being). 
		    I believe that I can only repay what they've done for me by assisting others in their journey in any way I can. 
		    Please don't hesitate to reach out!
              </p>
		  
<!--             <br><br>
	    <strong>Mentors:</strong> 
	    <br>    -->
	    <ul>
		  <li>David Kosbie - CMU CSD</li>
		  <li>Mark Stehlik - CMU CSD </li>
		  <li>Louis-Phillipe Morency - CMU LTI</li>
		  <li>Chaitanya Ahuja - CMU LTI </li>
		  <li>Ben Eysenbach - CMU MLD </li>
	    </ul>
	    <br>
	    
    
       
          </td>
        </tr>
      </tbody></table>
	      
	      
	      

        



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Source: <a href="https://github.com/jonbarron/jonbarron_website">source code</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
